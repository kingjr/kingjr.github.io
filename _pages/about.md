---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---


I am a CNRS researcher at [√âcole Normale Sup√©rieure](https://lsp.dec.ens.fr/fr) currently detached to [Meta AI](https://ai.facebook.com/), where I lead the Brain & AI team.
This team is dedicated to identify the brain and computational bases of human intelligence, with a focus on language.
For this, we develop deep learning models to decode and analyze intracranial recordings, magneto-encephalography and functional magnetic resonance imaging.

<a href='https://twitter.com/JeanRemiKing/status/1533720262344073218'>
<img src="/images/millet.gif" />
</a>

Press
====
- [Pour la Science](https://www.pourlascience.fr/sd/informatique/quand-les-ia-miment-l-activite-cerebrale-24554.php): Quand les IA miment l‚Äôactivit√© c√©r√©brale.
- [La Recherche](https://www.larecherche.fr/decrypter-les-reseaux-du-langage-dans-le-cerveau): D√©crypter les r√©seaux du langage dans le cerveau.
- [Time Magazine](https://time.com/6210261/meta-ai-brains-speech/): Meta Is Building AI That Reads Brainwaves. The Reality, So Far, Is Messy.
- [Meta AI](https://ai.facebook.com/blog/ai-speech-brain-activity/): Using AI to decode speech from brain activity.
- [Towards Data Science Podcast](https://towardsdatascience.com/does-the-brain-run-on-deep-learning-3fbaf20e9d12): Does the brain run on deep learning?
- [Quanta Magazine](https://www.quantamagazine.org/self-taught-ai-shows-similarities-to-how-the-brain-works-20220811/): Self-Taught AI Shows Similarities to How the Brain Works.
- [Meta AI](https://ai.facebook.com/blog/studying-the-brain-to-build-ai-that-processes-language-as-people-do/): Studying the brain to build AI that processes language as people do.
- [Ecole Normale Sup√©rieure](https://www.ens.psl.eu/actualites/jean-remi-king-entre-ia-et-neurosciences): Jean-R√©mi King, entre I.A. et neurosciences ([video](https://www.youtube.com/watch?v=KH2vUuU6bWo))
- [New York Times](https://www.nytimes.com/2019/06/26/health/brain-injury-eeg-consciousness.html): ‚ÄòIt‚Äôs Gigantic‚Äô: A New Way to Gauge the Chances for Unresponsive Patients
- [CBC News](https://www.cbc.ca/news/science/brain-subliminal-images-1.3879282): Your brain registers more than you think you see, NYU researchers find.


Our work summarized in short visual threads
====

<details open markdown=block style="background-color: rgb(250, 250, 250);">
<summary markdown=span>
Decoding speech from non-invasive brain recordings, üëá
<i>[arXiv](https://arxiv.org/abs/2208.12266)</i> 2022
</summary>
[D√©fossez](https://ai.honu.io/), [Caucheteux](https://charlottecaucheteux.github.io/), Kabeli, Rapin & [King](https://kingjr.github.io/)
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">‚ÄúDecoding speech from non-invasive brain recordings‚Äù,<br><br>Our latest study (on 169 participants!), by <a href="https://twitter.com/honualx?ref_src=twsrc%5Etfw">@honualx</a> and our wonderful team <a href="https://twitter.com/MetaAI?ref_src=twsrc%5Etfw">@MetaAI</a><br><br>- paper: <a href="https://t.co/QiB7Io8af8">https://t.co/QiB7Io8af8</a><br>- blog: <a href="https://t.co/H2W4prbbuD">https://t.co/H2W4prbbuD</a><br>- illustrated summary: belowüëá <a href="https://t.co/39eMnJ4IDv">pic.twitter.com/39eMnJ4IDv</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1564964019965927424?ref_src=twsrc%5Etfw">August 31, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details open markdown=block>
<summary markdown=span>Toward a realistic model of speech processing in the brain with self-supervised learning, <i>[arXiv](https://arxiv.org/abs/2206.01685)</i> 2022
</summary>
[Millet](https://jamju.github.io/)\*, [Caucheteux](https://charlottecaucheteux.github.io/)\*, Orhan, [Boubenec](https://lsp.dec.ens.fr/en/member/616/yves-boubenec), [Gramfort](https://alexandre.gramfort.net/), [Dunbar](http://individual.utoronto.ca/ewan_dunbar/), [Pallier](https://www.pallier.org/) & [King](https://kingjr.github.io/)
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">üî•Preprint out: <br><br>`Toward a realistic model of speech processing in the brain with self-supervised learning‚Äô:<a href="https://t.co/rJH6t6H6sm">https://t.co/rJH6t6H6sm</a><br><br>by J. [Millet](https://jamju.github.io/)*, <a href="https://twitter.com/c_caucheteux?ref_src=twsrc%5Etfw">@c_caucheteux</a>* and our wonderful team:<br><br>The 3 main results summarized below üëá <a href="https://t.co/mdrJpbrb3M">pic.twitter.com/mdrJpbrb3M</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1533720262344073218?ref_src=twsrc%5Etfw">June 6, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Brains and algorithms partially converge in natural language processing,
<i>[bioRxiv](https://www.biorxiv.org/content/10.1101/2020.07.03.186288v1.full.pdf)</i> 2020,
<i>[Nature Communications Biology](https://www.nature.com/articles/s42003-022-03036-1)</i> 2022
</summary>
[Caucheteux](https://charlottecaucheteux.github.io/) & [King](https://kingjr.github.io/)
<blockquote class="twitter-tweet" data-dnt="true"><p lang="en" dir="ltr">üéâPaper out: ‚ÄòBrains and algorithms partially converge in natural language processing‚Äô<br> <br>by <a href="https://twitter.com/c_caucheteux?ref_src=twsrc%5Etfw">@c_caucheteux</a>, and now freely available at Nature <a href="https://twitter.com/CommsBio?ref_src=twsrc%5Etfw">@CommsBio</a>:<a href="https://t.co/MpenOUaKwS">https://t.co/MpenOUaKwS</a><br> <br>The summary thread below üëá <a href="https://t.co/gMruZgGIOv">pic.twitter.com/gMruZgGIOv</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1496425017474695169?ref_src=twsrc%5Etfw">February 23, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Long-range and hierarchical language predictions in brains and algorithms,
<i>[arXiv](https://arxiv.org/abs/2111.14232)</i> 2021
</summary>
[Caucheteux](https://charlottecaucheteux.github.io/), [Gramfort](https://alexandre.gramfort.net/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b>We track language predictions in the brain and show that, unlike modern algorithms, they are hierarchical and apply to a variety of temporal scopes.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">‚ÄòLong-range and hierarchical language predictions in brains and algorithms‚Äô<br> <br>Check-out our latest paper <a href="https://t.co/rwfVCVLRWA">https://t.co/rwfVCVLRWA</a> by <a href="https://twitter.com/c_caucheteux?ref_src=twsrc%5Etfw">@c_caucheteux</a> <a href="https://twitter.com/agramfort?ref_src=twsrc%5Etfw">@agramfort</a> <a href="https://twitter.com/JeanRemiKing?ref_src=twsrc%5Etfw">@JeanRemiKing</a><br> <br>tl;dr: Unlike deep language models, the brain makes long-range &amp; hierarchical predictions<br> <br>Thread belowüëá <a href="https://t.co/iP0BEYBjip">pic.twitter.com/iP0BEYBjip</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1465716332264103940?ref_src=twsrc%5Etfw">November 30, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>

<details markdown=block>
<summary markdown=span>
Model-based analysis of brain activity reveals the hierarchy of language,
<i>[EMNLP](https://hal.inria.fr/hal-03361430)</i> 2021
</summary>

[Caucheteux](https://charlottecaucheteux.github.io/), [Gramfort](https://alexandre.gramfort.net/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b>We show how deep language algorithms help reveal the hierarchical organization of language integration in the brain.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Model-based analysis of brain activity reveals the hierarchy of language&quot;<br><br>Our EMNLP paper by <a href="https://twitter.com/c_caucheteux?ref_src=twsrc%5Etfw">@c_caucheteux</a> <a href="https://twitter.com/agramfort?ref_src=twsrc%5Etfw">@agramfort</a> &amp; myself is out: <a href="https://t.co/BxvrbZNkPt">https://t.co/BxvrbZNkPt</a><br><br>It shows (w/ emoji-based equations!) how deepnets can efficiently recover the language hierarchy in the<br><br>Summaryüëá<br>1/7 <a href="https://t.co/3QOcTfsivu">pic.twitter.com/3QOcTfsivu</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1447909791435825159?ref_src=twsrc%5Etfw">October 12, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Deep language algorithms predict semantic comprehension from brain activity,
<i>[Nature Scientific Report](https://www.nature.com/articles/s41598-022-20460-9)</i> 2022
</summary>

[Caucheteux](https://charlottecaucheteux.github.io/), [Gramfort](https://alexandre.gramfort.net/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b>The more we understand text, the more our brain responds like GPT-2.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Our latest paper is out: <br><br>GPT-2‚Äôs activations predict the degree of semantic comprehension in the human brain, by <a href="https://twitter.com/c_caucheteux?ref_src=twsrc%5Etfw">@c_caucheteux</a>, <a href="https://twitter.com/agramfort?ref_src=twsrc%5Etfw">@agramfort</a> &amp; <a href="https://twitter.com/JeanRemiKing?ref_src=twsrc%5Etfw">@JeanRemiKing</a><a href="https://t.co/Xjc8IaXT64">https://t.co/Xjc8IaXT64</a><br> <br>The summary thread below üëá <br>1/8 <a href="https://t.co/GF39doySMu">pic.twitter.com/GF39doySMu</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1402611813032857612?ref_src=twsrc%5Etfw">June 9, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Disentangling Syntax and Semantics in the Brain with Deep Networks,
<i>[ICML](http://proceedings.mlr.press/v139/caucheteux21a.html)</i> 2021
</summary>

[Caucheteux](https://charlottecaucheteux.github.io/), [Gramfort](https://alexandre.gramfort.net/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b>The similarity between deep nets and the brain allow us to decompose syntax and semantics in the brain.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Disentangling Syntax and Semantics in the Brain with Deep Networks&quot;<br><br>Go check out our latest <a href="https://twitter.com/icmlconf?ref_src=twsrc%5Etfw">@icmlconf</a> paper : <a href="https://t.co/4YPK7vJRsJ">https://t.co/4YPK7vJRsJ</a><br>by <a href="https://twitter.com/c_caucheteux?ref_src=twsrc%5Etfw">@c_caucheteux</a>, <a href="https://twitter.com/agramfort?ref_src=twsrc%5Etfw">@agramfort</a> &amp; <a href="https://twitter.com/JeanRemiKing?ref_src=twsrc%5Etfw">@JeanRemiKing</a><br><br>The summary thread below üëá <a href="https://t.co/v0kxjtBtVP">pic.twitter.com/v0kxjtBtVP</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1418216893182185473?ref_src=twsrc%5Etfw">July 22, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Inductive biases, pretraining and fine-tuning jointly account for brain responses to speech,
<i>[arXiv](https://arxiv.org/abs/2103.01032)</i> 2021
</summary>

[Millet](https://jamju.github.io/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b>Do convolutional networks process speech sounds like our brains does? Short answer: yes, even without training; but training helps.

<blockquote class="twitter-tweet" data-lang="en" data-theme="light"><p lang="en" dir="ltr">Do convolutional networks process speech sounds like our brains does?<br><br>Check out our latest study with Juliette [Millet](https://jamju.github.io/): <a href="https://t.co/dcupYxSxKA">https://t.co/dcupYxSxKA</a><br><br>Here is the summary thread üëá: 1/n <a href="https://t.co/LI6kr8PY9j">pic.twitter.com/LI6kr8PY9j</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1369230423545548807?ref_src=twsrc%5Etfw">March 9, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Deep Recurrent Encoder: A scalable end-to-end network to model magneto-encephalography at scale,
<i>[NBDT](https://hal.inria.fr/hal-03808304)</i> 2022
</summary>

[Chehab](https://l-omar-chehab.github.io/)\*, [D√©fossez](https://ai.honu.io/)\*, [Loiseau](https://loiseaujc.github.io/), [Gramfort](https://alexandre.gramfort.net/) & [King](https://kingjr.github.io/), <i>arXiv</i> 2021

<b>tl;dr:</b> We propose a new end-to-end architecture to encode MEG brain signals. It outperforms standard pipelines by a 3X.

<blockquote class="twitter-tweet" data-lang="en" data-theme="light"><p lang="en" dir="ltr">Deep learning improves the analysis of time-resolved brain signals by ... 3Ô∏è‚É£ folds!<br> <br>Check out our latest paper by <a href="https://twitter.com/lomarchehab?ref_src=twsrc%5Etfw">@lomarchehab</a>*, <a href="https://twitter.com/honualx?ref_src=twsrc%5Etfw">@honualx</a>*, <a href="https://twitter.com/loiseau_jc?ref_src=twsrc%5Etfw">@loiseau_jc</a>, and <a href="https://twitter.com/agramfort?ref_src=twsrc%5Etfw">@agramfort</a>:<br> <a href="https://t.co/QxTxoySnBs">https://t.co/QxTxoySnBs</a><br> <br>Below is the summary thread üëá <a href="https://t.co/h1WcoGm7UD">pic.twitter.com/h1WcoGm7UD</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1379775034579947520?ref_src=twsrc%5Etfw">April 7, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></details>


<details markdown=block>
<summary markdown=span>
Bifurcation in brain dynamics reveals a signature of conscious processing independent of report,
<i>[Nature Communications](https://www.nature.com/articles/s41467-021-21393-z)</i> 2021
</summary>

[Sergent](https://clairesergent1.wixsite.com/sergentlab/claire-sergent), Corazzol, Labouret, Stockart, Wexler, [King](https://kingjr.github.io/), [Meyniel](https://florentmeyniel.weebly.com/) & [Pressnitzer](http://audition.ens.fr/dp/)

<b>tl;dr:</b> We show with EEG that the conscious access follows an all-or-none dynamics even without report.

<blockquote class="twitter-tweet" data-lang="en" data-theme="light"><p lang="en" dir="ltr">Most work on the neural basis of consciousness relies on self-report, however <a href="https://twitter.com/MmeJeanserre?ref_src=twsrc%5Etfw">@MmeJeanserre</a>, <a href="https://twitter.com/JeanRemiKing?ref_src=twsrc%5Etfw">@JeanRemiKing</a> et al. suggest bifurcation in EEG brain dynamics may reflect an independent signature of conscious perception <a href="https://twitter.com/Univ_Paris?ref_src=twsrc%5Etfw">@Univ_Paris</a> <a href="https://twitter.com/Cognition_ENS?ref_src=twsrc%5Etfw">@Cognition_ENS</a> <a href="https://twitter.com/mne_python?ref_src=twsrc%5Etfw">@mne_python</a> <a href="https://t.co/nHMPaSVxnU">https://t.co/nHMPaSVxnU</a> <a href="https://t.co/n4TXgh2XNt">pic.twitter.com/n4TXgh2XNt</a></p>&mdash; Nature Communications (@NatureComms) <a href="https://twitter.com/NatureComms/status/1363133556080316417?ref_src=twsrc%5Etfw">February 20, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></details>



<details markdown=block>
<summary markdown=span>
Back-to-back regression: Disentangling the influence of correlated factors from multivariate observations,
<i>[Neuroimage](https://www.sciencedirect.com/science/article/pii/S1053811920305140)</i> 2020
</summary>

[King](https://kingjr.github.io/), Charton, [Lopez-Paz](http://lopezpaz.org/) & Oquab

<b>tl;dr:</b> We introduce a simple method to combine the advantages of decoding
and encoding analyses.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Back-to-back regression: Disentangling the influence of correlated factors from multivariate observations.<br><br>Our latest paper with <a href="https://twitter.com/f_charton?ref_src=twsrc%5Etfw">@f_charton</a>, David Lopez Paz &amp; Maxime Oquab at <a href="https://twitter.com/facebookai?ref_src=twsrc%5Etfw">@facebookai</a> is now freely available at Neuroimage: <a href="https://t.co/2hBgODEeAw">https://t.co/2hBgODEeAw</a><br><br>Here&#39;s the summary thread ‚§µÔ∏è <a href="https://t.co/i1ZLF2dZ5e">pic.twitter.com/i1ZLF2dZ5e</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1281164558141403137?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
COVID-19: the promises and pitfalls of Machine Learning,
<i>[Nature Machine Intelligence](https://www.nature.com/articles/s42256-020-0181-6)</i> 2020
</summary>


Peiffer-Smadja, Maatoug, Lescure, D‚ÄôOrtenzio, [Pineau](https://www.cs.mcgill.ca/~jpineau/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b> We're teaming up with the AP-HP hospital to review the
promises and pitfalls of Machine Learning.

<blockquote class="twitter-tweet" data-theme="light"><br><br>&quot;<a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> for <a href="https://twitter.com/hashtag/COVID%E3%83%BC19?src=hash&amp;ref_src=twsrc%5Etfw">#COVID„Éº19</a> needs global collaboration and data-sharing&quot;<br><br>üëâ<a href="https://t.co/ouY7MYX59p">https://t.co/ouY7MYX59p</a><br><br> <a href="https://twitter.com/hashtag/ArtificialIntelligence?src=hash&amp;ref_src=twsrc%5Etfw">#ArtificialIntelligence</a> <a href="https://twitter.com/hashtag/SARSCoV2?src=hash&amp;ref_src=twsrc%5Etfw">#SARSCoV2</a> <a href="https://t.co/lZsZh8Hqvm">pic.twitter.com/lZsZh8Hqvm</a></p>&mdash; Nathan Peiffer-Smadja (@nathanpsmad) <a href="https://twitter.com/nathanpsmad/status/1263862383308689409?ref_src=twsrc%5Etfw">May 22, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></details>


<details markdown=block>
<summary markdown=span>
Neural dynamics of phoneme sequences reveal position-invariant code for content and order,
<i>[Nature Communications](https://www.nature.com/articles/s41467-022-34326-1)</i> 2022
</summary>

[Gwilliams](https://lauragwilliams.github.io/), [King](https://kingjr.github.io/), [Marantz](https://wp.nyu.edu/morphlab/alec-marantz/) & [Poeppel](https://as.nyu.edu/faculty/david-poeppel.html)

<b>tl;dr:</b> Decoding the neural dynamics underlying phonetic representations shows how
the brain can keep up multiple phonemes until the corresponding word is identified.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">our new paper &quot;Neural dynamics of phoneme sequencing&quot; is now on bioRxiv!<a href="https://t.co/jeTipPTXuf">https://t.co/jeTipPTXuf</a><br><br>conducted with dream-team <a href="https://twitter.com/JeanRemiKing?ref_src=twsrc%5Etfw">@JeanRemiKing</a> <a href="https://twitter.com/AlecMarantz?ref_src=twsrc%5Etfw">@AlecMarantz</a> <a href="https://twitter.com/davidpoeppel?ref_src=twsrc%5Etfw">@davidpoeppel</a>, we use MEG to study how phonemes are processed in continuous naturalistic speech<br><br>short summary in thread below:<br>1/8 <a href="https://t.co/yT5bN2PfHw">pic.twitter.com/yT5bN2PfHw</a></p>&mdash; Laura Gwilliams (@GwilliamsL) <a href="https://twitter.com/GwilliamsL/status/1247216879992782848?ref_src=twsrc%5Etfw">April 6, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
The Human Brain Encodes a Chronicle of Visual Events at Each Instant of Time Through the Multiplexing of Traveling Waves,
<i>[Journal of Neuroscience](https://www.jneurosci.org/content/41/34/7224.abstract)</i> 2021
</summary>

[Wyart](https://sites.google.com/site/valentinwyart/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b> We measure brain responses to image sequences, and show how the brain recruits a hierarchy of neural processes in order to efficiently represents multiple snapshots of the past. [Check-out our tweet thread for the illustrated summary](https://twitter.com/JeanRemiKing/status/1196808278929526786?ref_src=twsrc%5Etfw)

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;The Human Brain encodes a Chronicle of Visual Events at each Instant of Time&quot;, by <a href="https://twitter.com/valentinwyart?ref_src=twsrc%5Etfw">@valentinwyart</a> and I: the tl;dr thread: <a href="https://t.co/YfLLZ1ZStr">https://t.co/YfLLZ1ZStr</a> <a href="https://t.co/iySGP52al9">pic.twitter.com/iySGP52al9</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1196808278929526786?ref_src=twsrc%5Etfw">November 19, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</details>


<details markdown=block>
<summary markdown=span>
Recurrent Processes Emulate a Cascade of Hierarchical Decisions,
<i>[eLife](https://elifesciences.org/articles/56603)</i> 2020
</summary>

[Gwilliams](https://lauragwilliams.github.io/) & [King](https://kingjr.github.io/)

<b>tl;dr:</b> When an image is ambiguous, the brain slowly recruits a hierarchy of recurrent processes to generate categorical percepts. [Check-out our tweet thread for the illustrated summary](https://twitter.com/JeanRemiKing/status/1195380648560615425?ref_src=twsrc%5Etfw)

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">0/9: &quot;Recurrent Processes Emulate a Cascade of Hierarchical Decisions&quot;, by <a href="https://twitter.com/GwilliamsL?ref_src=twsrc%5Etfw">@GwilliamsL</a> and I, the tl;dr thread:</p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1195380469031792641?ref_src=twsrc%5Etfw">November 15, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">3/9 Their average brain response confirm a fast feedforward recruitment of their visual hierarchies <a href="https://t.co/Y39WYwJ2Yx">pic.twitter.com/Y39WYwJ2Yx</a></p>&mdash; Jean-R√©mi King (@JeanRemiKing) <a href="https://twitter.com/JeanRemiKing/status/1195380648560615425?ref_src=twsrc%5Etfw">November 15, 2019</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
